# prompt-evaluator

├── .github
  ├── af-config
    ├── license-header.config.json
    ├── LICENSE_HEADER
    └── repo-analyzer.config.json
  ├── repo-analysis-output
    ├── dependencies.json
    ├── dependencies.md
    ├── file-summaries.json
    ├── file-summaries.md
    ├── SUMMARY.md
    ├── tree.json
    └── tree.md
  └── workflows
    └── af_maintenance.yml
├── docs
  └── datasets.md
├── examples
  ├── datasets
    ├── sample.jsonl
    └── sample.yaml
  ├── rubrics
    ├── code_review.json
    ├── content_quality.yaml
    └── default.yaml
  ├── .gitkeep
  ├── input.txt
  └── system_prompt.txt
├── src
  └── prompt_evaluator
    ├── __init__.py
    ├── cli.py
    ├── config.py
    ├── models.py
    └── provider.py
├── tests
  ├── __init__.py
  ├── test_aggregation.py
  ├── test_basic.py
  ├── test_config_models.py
  ├── test_dataset_loader.py
  ├── test_evaluate_single_cli.py
  ├── test_generate_cli.py
  ├── test_judge_models.py
  ├── test_rubric_cli.py
  └── test_rubric_models.py
├── .gitignore
├── LICENSE
├── pyproject.toml
└── README.md