# prompt-evaluator

├── .github
  ├── af-config
    ├── license-header.config.json
    ├── LICENSE_HEADER
    └── repo-analyzer.config.json
  ├── repo-analysis-output
    ├── dependencies.json
    ├── dependencies.md
    ├── file-summaries.json
    ├── file-summaries.md
    ├── SUMMARY.md
    ├── tree.json
    └── tree.md
  └── workflows
    └── af_maintenance.yml
├── docs
  ├── datasets.md
  └── reporting.md
├── examples
  ├── datasets
    ├── sample.jsonl
    └── sample.yaml
  ├── rubrics
    ├── code_review.json
    ├── content_quality.yaml
    └── default.yaml
  ├── run-artifacts
    ├── comparison-sample.json
    ├── index.json
    └── run-sample.json
  ├── .gitkeep
  ├── input.txt
  └── system_prompt.txt
├── src
  └── prompt_evaluator
    ├── reporting
      ├── __init__.py
      ├── compare_report.py
      └── run_report.py
    ├── __init__.py
    ├── cli.py
    ├── comparison.py
    ├── config.py
    ├── dataset_evaluation.py
    ├── models.py
    └── provider.py
├── tests
  ├── __init__.py
  ├── test_aggregation.py
  ├── test_basic.py
  ├── test_compare_report.py
  ├── test_compare_runs_cli.py
  ├── test_comparison.py
  ├── test_config_models.py
  ├── test_dataset_evaluation.py
  ├── test_dataset_loader.py
  ├── test_evaluate_dataset_cli.py
  ├── test_evaluate_single_cli.py
  ├── test_generate_cli.py
  ├── test_judge_models.py
  ├── test_prompt_version_metadata.py
  ├── test_rubric_cli.py
  ├── test_rubric_models.py
  └── test_run_report.py
├── .gitignore
├── LICENSE
├── pyproject.toml
└── README.md